{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "from sklearn import cluster\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from library import Cloud "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomly selecting n cloud images of each type\n",
    "sample_percent = 0.1\n",
    "sample_size = 40\n",
    "\n",
    "# Path leading to all cloud data\n",
    "path = \"C:\\\\Users\\\\peter\\\\Documents\\\\Cloud Project\\\\Data\\\\swimcat\\\\\"\n",
    "cloud_types = [\"A-sky\",\"B-pattern\",\"C-thick-dark\",\"D-thick-white\",\"E-veil\"]\n",
    "\n",
    "# initializing dict to store randomly n selected training images \n",
    "train_set = {}\n",
    "test_set = {}\n",
    "# Iterating over cloud types \n",
    "for ctype in cloud_types:\n",
    "    fin = path + ctype +\"\\\\images\\\\\"\n",
    "    \n",
    "    # Reading all images of cloud type ctype from path fin + ctype\n",
    "    allimgs = os.listdir(fin)\n",
    "    \n",
    "    # total number of images\n",
    "    n_imgs = len(allimgs)\n",
    "    \n",
    "    # number of images to select\n",
    "    n_sample = int(round(n_imgs*sample_percent,1))\n",
    "    \n",
    "    # shuffling list of images, slicing to get n images\n",
    "    samples = allimgs[:sample_size]\n",
    "    \n",
    "    # Note that np.random.shuffle changes in place (ugh)\n",
    "    np.random.shuffle(samples)\n",
    "    \n",
    "    # Dict key set to -> ctype, dict value -> samples\n",
    "    train_set[ctype] = samples.copy()\n",
    "    test_set[ctype] = allimgs[sample_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell reads in all sets [1-5] of s-parameters used in the paper.\n",
    "# S-parameters are stored in a .txt file. \n",
    "\n",
    "os.listdir(os.getcwd())\n",
    "\n",
    "sparams = {}\n",
    "\n",
    "with open('sparams.txt') as fin:\n",
    "    for line in fin:\n",
    "        split = line.split(' ')\n",
    "        \n",
    "        params = [val.split(',') for val in split[1:]]\n",
    "        params = [[float(num) for num in val.split(',')] for val in split[1:]]\n",
    "        \n",
    "        sparams[split[0]] = params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell creates the cluster centers for each set of parameters. The\n",
    "# clusters are computed using mini-batch kmeans, (~2 minutes, 5 imgs, for all sets)\n",
    "# as standard kmeans takes too long (~30 minutes). \n",
    "# \n",
    "# Results are stored as binary files via pickle, and are also returned and \n",
    "# stored in a dictionary. \n",
    "\n",
    "centers = {}\n",
    "kmeans_obj = {}\n",
    "temp_texton_dict = {}\n",
    "for key in list(sparams.keys()):\n",
    "    kmeans_obj[key] = Cloud.get_textons(sparams[key], train_set, cloud_types, sample_size, \n",
    "                                                      name=key, path='..//Data//swimcat//')\n",
    "    centers[key] = kmeans_obj[key].cluster_centers_\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell creates a nested dictionary of histograms for each set \n",
    "# of Gabor (sfilter) kernel parameters. Clustering is done using\n",
    "# the kmeans Clustering objects defined in the previous cell. Histograms \n",
    "# are averaged as data is stored - output from each individual image is \n",
    "# NOT stored. \n",
    "\n",
    "texton_dict = {}\n",
    "set_dict = {}\n",
    "\n",
    "for key in list(kmeans_obj.keys()):\n",
    "    texton_dict[key] = {}\n",
    "    for ctype in cloud_types:\n",
    "        for image in train_set[ctype]:\n",
    "\n",
    "            output_array = np.zeros((125*125, len(sparams[key])))\n",
    "            imgpath = path + ctype + \"\\\\images\\\\\" + image\n",
    "            img = Cloud.scale_minmax(cv2.imread(imgpath, 1)[::, ::, 0]/cv2.imread(imgpath, 1)[::, ::, 2])\n",
    "            \n",
    "            dim = 0\n",
    "\n",
    "            for param in sparams[key]:\n",
    "                sfilter = Cloud.sfilter(param[0], param[1])\n",
    "\n",
    "                output_array[::,dim] = np.reshape(Cloud.scale_minmax(cv2.filter2D(img, -1, sfilter)), (125 * 125, ))\n",
    "                dim += 1\n",
    "            \n",
    "            prediction = np.bincount(kmeans_obj[key].predict(output_array))\n",
    "            \n",
    "            if len(prediction) < 30:\n",
    "                prediction = np.append(prediction, [0]*(30 - len(prediction)))                    \n",
    "            \n",
    "            if ctype not in texton_dict[key].keys():\n",
    "                texton_dict[key][ctype] = prediction\n",
    "            else:\n",
    "                # Averaging output distributions\n",
    "                texton_dict[key][ctype] = (texton_dict[key][ctype] + prediction)/2.0\n",
    "            \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.5, 0.0]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nearest_hist(clouddict, filt_img):\n",
    "\n",
    "    def chidiff(truth, img):\n",
    "        return np.sum(np.power((truth - img), 2)/(2*(truth + img)))\n",
    "\n",
    "    closest = None\n",
    "    dist = None\n",
    "    for key in list(clouddict.keys()):\n",
    "        if closest is None:\n",
    "            closest = key\n",
    "            dist = chidiff(clouddict[key], filt_img)\n",
    "\n",
    "        elif chidiff(clouddict[key], filt_img) < dist:\n",
    "            closest = key\n",
    "            dist = chidiff(clouddict[key], filt_img)\n",
    "\n",
    "    return closest\n",
    "\n",
    "results = []\n",
    "for key in list(texton_dict.keys())[2:3]:\n",
    "    for ctype in list(test_set.keys()):\n",
    "        for image in train_set[ctype]:\n",
    "\n",
    "            output_array = np.zeros((125*125, len(sparams[key])))\n",
    "            imgpath = path + ctype + \"\\\\images\\\\\" + image\n",
    "            img = Cloud.scale_minmax(cv2.imread(imgpath, 1)[::, ::, 0]/cv2.imread(imgpath, 1)[::, ::, 2])\n",
    "            \n",
    "            dim = 0\n",
    "\n",
    "            for param in sparams[key]:\n",
    "                sfilter = Cloud.sfilter(param[0], param[1])\n",
    "\n",
    "                output_array[::,dim] = np.reshape(Cloud.scale_minmax(cv2.filter2D(img, -1, sfilter)), (125 * 125, ))\n",
    "                dim += 1\n",
    "            \n",
    "            prediction = np.bincount(kmeans_obj[key].predict(output_array))\n",
    "            \n",
    "            if len(prediction) == 28:\n",
    "                prediction = np.append(prediction, (0,0)) \n",
    "            \n",
    "            if len(prediction) == 29:\n",
    "                prediction = np.append(prediction, 0)  \n",
    "        \n",
    "            results.append((ctype, nearest_hist(texton_dict[key], prediction)))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sfilter[35,35]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.75\n"
     ]
    }
   ],
   "source": [
    "corr = 0\n",
    "for result in results:\n",
    "    if result[0] == result[1]:\n",
    "        corr += 1\n",
    "        \n",
    "print(corr/len(results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in texton_dict['set1:']:\n",
    "    plt.figure()\n",
    "    plt.title(key)\n",
    "    plt.bar(list(range(0,30)),texton_dict['set1:'][key])\n",
    "    plt.ylim((0, 1400))\n",
    "    plt.savefig(key + \".png\", dpi=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texon_dict = {}\n",
    "\n",
    "for point in responses: \n",
    "    for cluster in kmeans.cluster_centers_:\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_obj"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "train_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgpath = \"C:\\\\Users\\peter\\Documents\\Cloud Project\\Data\\swimcat\\B-pattern\\images\\\\B_1img.png\"\n",
    "a = gabor_fn(1,0,1,0,1)\n",
    "img = scale_minmax(cv2.imread(imgpath,1))\n",
    "test = scale_minmax(cv2.filter2D(img, -1, a))\n",
    "plt.figure(1)\n",
    "plt.imshow(test)\n",
    "plt.figure(2)\n",
    "plt.imshow(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texton_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "responses.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sfilter.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(gabor_fn(10,0,6,0,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sfilter(sigma, tau):\n",
    "\n",
    "    sigma_x = sigma\n",
    "    sigma_y = float(sigma)\n",
    "\n",
    "    # Bounding box\n",
    "    nstds = 3 # Number of standard deviation sigma\n",
    "    xmax = max(abs(nstds * sigma_x), abs(nstds * sigma_y))\n",
    "    xmax = np.ceil(max(1, xmax))\n",
    "    ymax = max(abs(nstds * sigma_x), abs(nstds * sigma_y))\n",
    "    ymax = np.ceil(max(1, ymax))\n",
    "    xmin = -xmax\n",
    "    ymin = -ymax\n",
    "    (y, x) = np.meshgrid(np.arange(ymin, ymax + 1), np.arange(xmin, xmax + 1))\n",
    "\n",
    "\n",
    "\n",
    "    gb = np.cos(np.sqrt(x**2 + y**2)*np.pi*tau/sigma)*np.exp(-(x**2 + y**2)/(2*sigma**2))\n",
    "    return gb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(sfilter(10, 4))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
