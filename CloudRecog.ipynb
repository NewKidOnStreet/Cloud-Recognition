{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "from sklearn import cluster\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from library import Cloud "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomly selecting n cloud images of each type\n",
    "sample_percent = 0.1\n",
    "sample_size = 40\n",
    "\n",
    "# Path leading to all cloud data\n",
    "path = \"C:\\\\Users\\\\peter\\\\Documents\\\\Cloud Project\\\\Data\\\\swimcat\\\\\"\n",
    "cloud_types = [\"A-sky\",\"B-pattern\",\"C-thick-dark\",\"D-thick-white\",\"E-veil\"]\n",
    "\n",
    "# initializing dict to store randomly n selected training images \n",
    "train_set = {}\n",
    "test_set = {}\n",
    "# Iterating over cloud types \n",
    "for ctype in cloud_types:\n",
    "    fin = path + ctype +\"\\\\images\\\\\"\n",
    "    \n",
    "    # Reading all images of cloud type ctype from path fin + ctype\n",
    "    allimgs = os.listdir(fin)\n",
    "    \n",
    "    # total number of images\n",
    "    n_imgs = len(allimgs)\n",
    "    \n",
    "    # number of images to select\n",
    "    n_sample = int(round(n_imgs*sample_percent,1))\n",
    "    \n",
    "    # shuffling list of images, slicing to get n images\n",
    "    samples = allimgs[:sample_size]\n",
    "    \n",
    "    # Note that np.random.shuffle changes in place (ugh)\n",
    "    np.random.shuffle(samples)\n",
    "    \n",
    "    # Dict key set to -> ctype, dict value -> samples\n",
    "    train_set[ctype] = samples.copy()\n",
    "    test_set[ctype] = allimgs[sample_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell reads in all sets [1-5] of s-parameters used in the paper.\n",
    "# S-parameters are stored in a .txt file. \n",
    "\n",
    "os.listdir(os.getcwd())\n",
    "\n",
    "sparams = {}\n",
    "\n",
    "with open('sparams.txt') as fin:\n",
    "    for line in fin:\n",
    "        split = line.split(' ')\n",
    "        \n",
    "        params = [val.split(',') for val in split[1:]]\n",
    "        params = [[float(num) for num in val.split(',')] for val in split[1:]]\n",
    "        \n",
    "        sparams[split[0]] = params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell creates the cluster centers for each set of parameters. The\n",
    "# clusters are computed using mini-batch kmeans, (~2 minutes, 5 imgs, for all sets)\n",
    "# as standard kmeans takes too long (~30 minutes). \n",
    "# \n",
    "# Results are stored as binary files via pickle, and are also returned and \n",
    "# stored in a dictionary. \n",
    "\n",
    "centers = {}\n",
    "kmeans_obj = {}\n",
    "temp_texton_dict = {}\n",
    "for key in list(sparams.keys()):\n",
    "    kmeans_obj[key] = Cloud.get_textons(sparams[key], train_set, cloud_types, sample_size, \n",
    "                                                      name=key, path='..//Data//swimcat//')\n",
    "    centers[key] = kmeans_obj[key].cluster_centers_\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (30,) (28,) ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-2a406c758a8e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     34\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m                 \u001b[1;31m# Averaging output distributions\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 36\u001b[1;33m                 \u001b[0mtexton_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mctype\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtexton_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mctype\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mprediction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;36m2.0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     37\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: operands could not be broadcast together with shapes (30,) (28,) "
     ]
    }
   ],
   "source": [
    "# This cell creates a nested dictionary of histograms for each set \n",
    "# of Gabor kernel parameters. Clustering is done using the kmeans\n",
    "# Clustering objects defined in the previous cell. Histograms are\n",
    "# averaged as data is stored - output from each individual image is \n",
    "# NOT stored. \n",
    "\n",
    "texton_dict = {}\n",
    "set_dict = {}\n",
    "\n",
    "for key in list(kmeans_obj.keys()):\n",
    "    texton_dict[key] = {}\n",
    "    for ctype in cloud_types:\n",
    "        for image in train_set[ctype]:\n",
    "\n",
    "            output_array = np.zeros((125*125, len(sparams[key])))\n",
    "            imgpath = path + ctype + \"\\\\images\\\\\" + image\n",
    "            img = Cloud.scale_minmax(cv2.imread(imgpath, 1))[::, ::, 0]\n",
    "            \n",
    "            dim = 0\n",
    "\n",
    "            for param in sparams[key]:\n",
    "                sfilter = Cloud.gabor_fn(param[0], 0, param[1], 0, 1)\n",
    "\n",
    "                output_array[::,dim] = np.reshape(Cloud.scale_minmax(cv2.filter2D(img, -1, sfilter)), (125 * 125, ))\n",
    "                dim += 1\n",
    "            \n",
    "            prediction = np.bincount(kmeans_obj[key].predict(output_array))\n",
    "            \n",
    "            if len(prediction) == 29:\n",
    "                prediction = np.append(prediction, 0)                    \n",
    "            \n",
    "            if ctype not in texton_dict[key].keys():\n",
    "                texton_dict[key][ctype] = prediction\n",
    "            else:\n",
    "                # Averaging output distributions\n",
    "                texton_dict[key][ctype] = (texton_dict[key][ctype] + prediction)/2.0\n",
    "            \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "results = []\n",
    "for key in list(texton_dict.keys())[0:1]:\n",
    "    for ctype in list(test_set.keys()):\n",
    "        for image in train_set[ctype]:\n",
    "\n",
    "            output_array = np.zeros((125*125, len(sparams[key])))\n",
    "            imgpath = path + ctype + \"\\\\images\\\\\" + image\n",
    "            img = Cloud.scale_minmax(cv2.imread(imgpath, 1))[::, ::, 0]\n",
    "            \n",
    "            dim = 0\n",
    "\n",
    "            for param in sparams[key]:\n",
    "                sfilter = Cloud.gabor_fn(param[0], 0, param[1], 0, 1)\n",
    "\n",
    "                output_array[::,dim] = np.reshape(Cloud.scale_minmax(cv2.filter2D(img, -1, sfilter)), (125 * 125, ))\n",
    "                dim += 1\n",
    "            \n",
    "            prediction = np.bincount(kmeans_obj[key].predict(output_array))\n",
    "            \n",
    "            if len(prediction) == 29:\n",
    "                prediction = np.append(prediction, 0)  \n",
    "        \n",
    "            results.append(ctype, Cloud.nearest_hist(texton_dict[key], prediction))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texton_dict[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in texton_dict['set1:']:\n",
    "    plt.figure()\n",
    "    plt.title(key)\n",
    "    plt.bar(list(range(0,30)),texton_dict['set1:'][key])\n",
    "    plt.ylim((0, 1400))\n",
    "    plt.savefig(key + \".png\", dpi=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texon_dict = {}\n",
    "\n",
    "for point in responses: \n",
    "    for cluster in kmeans.cluster_centers_:\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cloud_types"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "train_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgpath = \"C:\\\\Users\\peter\\Documents\\Cloud Project\\Data\\swimcat\\B-pattern\\images\\\\B_1img.png\"\n",
    "a = gabor_fn(1,0,1,0,1)\n",
    "img = scale_minmax(cv2.imread(imgpath,1))\n",
    "test = scale_minmax(cv2.filter2D(img, -1, a))\n",
    "plt.figure(1)\n",
    "plt.imshow(test)\n",
    "plt.figure(2)\n",
    "plt.imshow(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texton_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "responses.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
